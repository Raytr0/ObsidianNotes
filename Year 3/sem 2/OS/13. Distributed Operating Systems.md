### Distributed Systems
• Distributed system is a collection of processors that do not share a common clock. 
• These systems are often referred to as “nodes” on the network. 
• Each has their own memory, CPU, address, etc.
![[Pasted image 20250424222020.png|300]]
### Models of distributed systems
**Client server model — unreliable communication**
• Communication can suffer packet loss 
• Due to hardware faults and software errors (e.g. server buffer that received the request is full) 
• If a protocol does not deal with these packet losses, then it is an unreliable communication.
![[Pasted image 20250424222141.png]]
![[Pasted image 20250425010616.png]]
int sd = UDP_Open(10000); Opens a socket connection using sockets API.
int rc = UDP_Read(sd, &addr, message, BUFFER_SIZE); Fills the addrSnd struct with the hostname/port of the server.
rc = UDP_Write(sd, &addr, reply, BUFFER_SIZE); Sends the message
![[Pasted image 20250425010130.png]]
int sd = UDP_Open(10000); Opens a socket connection using sockets API. but server now
rc = UDP_Write(sd, &addr, reply, BUFFER_SIZE); Reads message by client and sends back
• Even in this simple example, we see manifestation of unreliable communication because 
• The client cannot distinguish if the reason for the wait for “goodbye world” response is a consequence of the following: 
	• The server never received it. 
	• The server sent it but the client never received it. 
	• The client can be waiting indefinitely
**Reliable communication with acknowledgement**
![[Pasted image 20250425011521.png]]
### Middleware in distributed systems
“Service oriented” systems — example CORBA
![[Pasted image 20250425011558.png]]
### Middleware vs Distributed OS
![[Pasted image 20250425011635.png]]
### Distributed systems characteristics
• Nodes in distributed OS are a collection of autonomous and cooperative computing elements. 
	• They can act independently of each other, but not ignore each other. 
	• The nodes can communicate with each other through messages over a TCP/IP or UDP channel. 
• The system should appear as a single coherent system. 
	• User should not have to think about which node their processes are running — distribution transparency. 
	• This means distributed systems need to deal with failure — if there is network failure, that does not achieve transparency.
### Distributed operating system
• In a distributed OS, the users should be able to access remote resources (data, processor, files. etc.) in the same way they access local resources. 
• This means that all these resources need to be migrated from one node to another as needed. 
• Contrast this with the design of the “middleware” based distributed systems where the required operations happened remotely through message passing.
### Types of processes in distributed operating systems
![[Pasted image 20250425012541.png]]
### Relation between divisible processes’ tasks
![[Pasted image 20250425014043.png]]
### Process migration
![[Pasted image 20250425014344.png]]
• Load balancing: strive to get equal load throughout the system in terms of metrics like CPU utilization. 
• Load sharing: Only move to help overburdened CPU
**Two components to process migration**
Information gathering The process distribution algorithm need to gather information about the current load of each node. Categorize each CPU as : overloaded, underloaded, suitably loaded.
Process selection. Need to consider: Communication delay. Will the turnaround time be faster if the task is migrated vs if the task is executed locally.
### Centralized vs Decentralized
![[Pasted image 20250425014749.png|250]]![[Pasted image 20250425014757.png|250]]
• Distributed OS needs to constantly make decisions of the nature we saw in the previous slide. One way to do this is to have one dedicated node have all the information about all nodes in the network of CPUs (Centralized algorithms) 
• Decentralized algorithms, on the other hand, needs to broadcast the local state constantly — “I am overburdened! help! Can a CPU help me”
### Distributed process scheduling
• There are two parts to process scheduling in distributed OS: 
• Local scheduling — this is same as scheduling a process locally in a CPU. Similar to the algorithms you have studied earlier in the course. The efficiency criteria is often CPU utilization/ average wait time, etc. 
• Global scheduling — how to distribute the tasks across the nodes and when to distribute. The efficiency criteria is to distribute load evenly. Finding optimal allocation of processes to nodes are often NP-hard. Therefore, suboptimal algorithms are often used.
![[Pasted image 20250425014931.png]]
![[Pasted image 20250425014948.png]]
### Virtualization — virtual machines and the cloud.
The key idea behind virtualization is to separate the tight coupling of OS to hardware. In other words, provide the OS with a “virtual machine”.
### Why use virtualization
• Rapid development of prototypes: You do not need to buy and install a new operating system if you just want to develop a prototype application on a particular OS. 
• Scalability: With very high computing power (and clusters), vendors can rent out machine hardware to multiple users with different guest OSs running on the same hardware. 
• Portability and availability of services: With virtualization it is possible to move a running application “live” from one hardware to another — something that was unimaginable before. This has revolutionized modern application development
![[Pasted image 20250425015140.png]]
### Type 1 and Type 2 Hypervisors
![[Pasted image 20250425015252.png]]
Type 1 is like an operating system whereas type 2 is almost like a regular process
It is very different.
### Hypervisor implementation — core concepts
![[Pasted image 20250425015431.png]]
Privileged instruction: Executing a privileged instruction will raise an exception in the physical machine.
Trap: 
	The physical CPU will raise an exception and hand over the control to the hypervisor
	But this needs support from the CPU. Because it needs to know that it has to trap to the hypervisor (since OS does not interact directly with the CPU) Starting late 2000s, most processors provide this support through “virtualization technology”
### Hardware Assisted Virtualization
Examples: (Type 1)Xen: used by Amazon EC2 (till 2017), Rackspace (Type 2)VMWare Fusion
### Binary translation
Examples: (Type 1)VMWare ESXi (Type 2)VMWare Workstation 1
![[Pasted image 20250425015720.png]]
### Paravirtualization
Examples: (Type 1)Xen 1.0 (Type 2)VirtualBox 5.0*
In paravirtualization, the guest OS is aware it’s running on a hypervisor rather than directly on hardware, allowing for cooperative interaction. The hypervisor provides _hypercalls_, similar to system calls, which the guest OS uses to access hardware services and privileged instructions. For instance, instead of directly accessing I/O controller registers, the guest OS makes hypercalls, and the hypervisor handles these through its own drivers.
![[Pasted image 20250425015850.png]]
### Containers
Examples: Docker
• Few points to note about hypervisors: they emulate the hardware. The isolation of different systems are achieved by isolation of two separate guest OSs. 
• But this also adds a lot of overhead. 
• Is it possible to achieve a lightweight isolation on a single OS running a single kernel? — Containers
Type 2 hypervisor vs container
![[Pasted image 20250425020011.png]]
### Containers in Linux
Kernel Control Groups
• Setup isolation of kernel level services, such as, processes, file systems, memory, IP addresses, etc into separate control groups (aka containers)
As an example, separate containers can have separate process hierarchy instead of a single on (as is typical in a non container based system) Similarly, separate containers will have separate mount points for the root file system.
![[Pasted image 20250425020134.png]]
### Live migration in hypervisors
![[Pasted image 20250425020114.png]]
